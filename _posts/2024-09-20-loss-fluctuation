---
title: Text and Typography
author: eunhye
date: 2020-08-08 11:33:00 +0800
categories: [Blogging, Demo]
tags: [typography, testing]
math: true
---

### ğŸ¤”Lossê°€ ê³„ì† fluctuate í•œë‹¤
#### ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ í™•ì¸í•´ì•¼í•  ìš”ì†Œë“¤
1. optimizer
2. learning rate sechduling
3. model capacity (=size)
4. data

> ì—¬ê¸°ì„œ (3) model capacityë‘ (4) dataê°€ overfittingì¸ì§€ underfittingì¸ì§€ë¥¼ ê²°ì •í•œë‹¤.
ë§Œì•½ ëª¨ë¸ì´ ë°ì´í„°ë³´ë‹¤ ë” í¬ë‹¤ë©´ underfitting ë°ì´í„°ë³´ë‹¤ ëª¨ë¸ì´ ë” í¬ë‹¤ë©´ underfittingì¸ ê²ƒì´ë‹¤.

#### Optimizer and Learning Rate
- **Adam optimizer**ì˜ ê²½ìš°, ì €ì ˆë¡œ lrë¥¼ ë°”ê¾¼ë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•´ì•¼í•œë‹¤.
- **Stimulated Annealing**
    - _ë‹´ê¸ˆì§ˆ ê¸°ë²•._ ì¦‰, ì˜¨ë„ê°€ ì˜¬ë¼ê°”ë‹¤ê°€ ë‚´ë ¤ê°€ë©´ì„œ í˜•íƒœê°€ ë³´ë‹¤ ì‰½ê²Œ ë³€í™”í•  ìˆ˜ ìˆ/ì—†ìŒì„ ì˜ë¯¸í•œë‹¤.
    - ì˜¨ë„ê°€ ì˜¬ë¼ê°„ë‹¤ (= lrì´ ë†’ë‹¤) : ë” ì‰½ê²Œ ë³€í™”í•œë‹¤ëŠ” ì˜ë¯¸. lrì´ ë” ë†’ìœ¼ë‹ˆê¹Œ, ë” í° ë³´í­ìœ¼ë¡œ ì›€ì§ì´ê¸°ì— ë³€í™”ê°€ ë” í¬ë‹¤. 
    - ë§Œì•½ lrê°€ í° ê°’ì—ì„œ ì‘ì€ ê°’ìœ¼ë¡œ ë³€í™”í•  ë•Œ lossì—ì„œ í° ê°ì†Œê°€ ìˆë‹¤ë©´, í° ë³´í­ìœ¼ë¡œ í•´ë‹¹ convex ì‚¬ì´ì—ì„œ ì§€ê·¸ì¬ê·¸ë¡œ í•™ìŠµë˜ë‹¤ê°€, ë³´í­ì´ ì¤„ì–´ë“œëŠ” ìˆœê°„ gobal minimumì— ê°€ê¹Œì›Œì¡Œë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤. 
    ![](https://velog.velcdn.com/images/pehye89/post/0bdcefff-1277-4d1f-9e65-e5dd3027fe20/image.png)

#### Model Capacity
- ë§Œì•½ ëª¨ë¸ì˜ lossê°€ ì¶©ë¶„íˆ ë‚®ì§€ ì•Šìœ¼ë©´ model capacityê°€ ë„ˆë¬´ ì‘ì•„ì„œ, ì¦‰, ë„ˆë¬´ ì‘ì€ ëª¨ë¸ë¡œ í•™ìŠµì„ ì‹œí‚¤ê³  ìˆì–´ì„œ ê·¸ëŸ´ ìˆ˜ ìˆë‹¤ _(ë˜ëŠ” ë°ì´í„°ì˜ ë…¸ì´ì¦ˆê°€ ë„ˆë¬´ ë§ì•„ì„œ ê·¸ëŸ° ê²ƒì¼ ìˆ˜ë„ ìˆë‹¤)_.
- ë” í° ëª¨ë¸ë¡œ í•™ìŠµì‹œì¼œë³´ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì´ ë  ìˆ˜ ìˆë‹¤. 
> ë‚´ê°€ í™•ì¸í•œ ë¬¸ì œì˜ ê²½ìš° ë” ë³µì¡í•œ ëª¨ë¸ì„ ì‚¬ìš©í–ˆì„ ë•Œ fluctuationì´ ì¤„ì—ˆê³  lossë„ ë” ë‚®ê²Œ ìˆ˜ë ´í–ˆë‹¤. 

#### Stochastic GD vs. "Regular" Gradient Descent
- Stochasticì˜ ì˜ë¯¸ : í™•ë¥ ë¡ ì 
- Stochasticì˜ ê²½ìš° ë°°ì¹˜ë¡œ ë‚˜ëˆ ì„œ ì¼ë¶€ì˜ ë°ì´í„°ë¡œ ë‹¤ìŒ ê²°ì •ì„ ë‚´ë¦°ë‹¤ë©´, ì¼ë°˜ì ì¸ gradient descentì˜ ê²½ìš° ëª¨ë“  ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ê²°ì •ì„ ë‚´ë¦¬ëŠ” ê²ƒì´ë‹¤.
- ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë§ì€ ë°ì´í„°ë¡œ í•™ìŠµì„ ì‹œí‚¬ê²½ìš° SGDë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. 

> _This iterative process continues with randomly selected training examples until the algorithm reaches a predetermined stopping point. This method is preferred when working with large datasets because it takes less time to process a single example than it does to process the entire dataset._ [(source)](https://medium.com/@seshu8hachi/stochastic-gradient-descent-vs-gradient-descent-exploring-the-differences-9c29698b3a9b) 

### ğŸ˜ì§ì ‘ ì‹œë„
- ì‚¬ìš©í•œ ëª¨ë¸ : PaddlePaddleì˜ [PaddleYOLOE+](https://github.com/PaddlePaddle/PaddleYOLO) (batch size : 16)
    - vibrant-wave-12 (ì£¼í™©ìƒ‰) : PP-YOLOE+_s
    - usual-violet-8 (ë³´ë¼ìƒ‰) : PP-YOLOE+_m 

![](https://velog.velcdn.com/images/pehye89/post/43b7d021-dc2d-423d-ac28-0a9a3806a686/image.png)
- ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë¥¼ í‚¤ì›Œì„œ ë‹¤ì‹œ ëŒë ¤ë´¤ì„ ë•Œ(ì£¼í™©ìƒ‰) ì „ë³´ë‹¤ëŠ”(ë³´ë¼ìƒ‰) í™•ì‹¤íˆ ë” ì•ˆì •ì ì¸ ì»¤ë¸Œë¥¼ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤.
![](https://velog.velcdn.com/images/pehye89/post/8c6811de-0eb4-473a-a680-fdac4382dcbe/image.png)


- ë˜ ìœ„ì—ì„œ ì–¸ê¸‰í•œ "ë³´í­ì´ í™• ì¤„ì–´ë“œëŠ” ìˆœê°„ lossì˜ ê¸‰ê²©í•œ ë³€í™”"ë¥¼ ì§ì ‘ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤.
- ì§€ê¸ˆ gpuì˜ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•´ì„œ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ë” í‚¤ìš°ê³  ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë¦¬ì§€ ëª»í•˜ëŠ” ìƒí™©ì´ ì•„ì‰¬ìš´ ë¶€ë¶„

